{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning and inference\n",
        "\n",
        "To train an Automatic Speech Recognition (ASR) model, the following inputs are required:\n",
        "\n",
        "1. **Audio Data with Corresponding Transcriptions**: A dataset of audio recordings paired with their textual transcriptions.  \n",
        "2. **Dictionary**: Defines the set of tokens over which the acoustic model predicts probabilities for each audio frame.  \n",
        "3. **Lexicon**: Maps words to sequences of tokens, enabling the conversion of text into token representations.\n",
        "\n",
        "#### Tools used:\n",
        "\n",
        "1. **Fairseq**: Used for fine-tuning pre-trained models, enabling efficient training of ASR systems with existing models and datasets.  \n",
        "2. **KenLM**: Utilized for building and integrating language models, which enhance the recognition accuracy by capturing the probabilities of word sequences.  \n",
        "3. **Flashlight**: Employed for decoding, providing a fast and flexible beam search decoder to predict text sequences from acoustic model outputs."
      ],
      "metadata": {
        "id": "R5YDzL8M9hSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the fairseq and the Ai4Bharat repo."
      ],
      "metadata": {
        "id": "4ZZAvIr4Qvxp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsCLcDGV8UZM"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/AI4Bharat/IndicWav2Vec.git\n",
        "!git clone https://github.com/pytorch/fairseq.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try different versions of pip that is compatible with pip\n",
        "# The latest version is likely to be uncompatible\n",
        "!pip install pip==23.1.2"
      ],
      "metadata": {
        "id": "cslgf90-9tUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/IndicWav2Vec\n",
        "!pip install packaging soundfile swifter -r w2v_inference/requirements.txt\n",
        "%cd .."
      ],
      "metadata": {
        "id": "LfJDX8cRBjzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Install fairseq"
      ],
      "metadata": {
        "id": "mzDP1mwu9-zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd fairseq\n",
        "!pip install --editable ./\n",
        "%cd"
      ],
      "metadata": {
        "id": "yeoIrfH899F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install other dependencies\n",
        "!pip install torch torchvision torchaudio soundfile torchaudio sentencepiece editdistance scikit-learn"
      ],
      "metadata": {
        "id": "RW6LTTZ6-E0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare the folder structure"
      ],
      "metadata": {
        "id": "f2npXoO1-Vz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir datasets\n",
        "!mkdir datasets/santali"
      ],
      "metadata": {
        "id": "3NASPpFU-LIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Download the dataset"
      ],
      "metadata": {
        "id": "E9LMFebt-nuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "mdyITeMg-qga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Anonymous linkes for train, valid, test\n",
        "!gdown 1kJSOB3hpAcYCLwpExR03A3TXQZvFkqfD\n",
        "!gdown 1QBz-IeMmgi2EPCa9endt0Zvwa3tLiuP2\n",
        "!gdown 1gfyDg464ExtaS8OdzcVxB5BQW1S3ZqKi"
      ],
      "metadata": {
        "id": "oc9Lwg5u-r4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract files\n",
        "!unzip train.zip -d datasets/santali/train\n",
        "!unzip test.zip -d datasets/santali/test\n",
        "!unzip valid.zip -d datasets/santali/valid"
      ],
      "metadata": {
        "id": "97NAmhge-1el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Download pre-trained model"
      ],
      "metadata": {
        "id": "_4stqS2H-9hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://indic-asr-public.objectstore.e2enetworks.net/aaai_ckpts/pretrained_models/indicw2v_base_pretrained.pt"
      ],
      "metadata": {
        "id": "AYXQXsJ4_A-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir checkpoint"
      ],
      "metadata": {
        "id": "Bl3hQsn9_EwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Start fine-tuning"
      ],
      "metadata": {
        "id": "zxBSWIWy_prD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Run this cell if some fairseq module not found error shows up\n",
        "## Just a bypass, reset PYTHONPATH after finetuning\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] = \"/content/fairseq/\"\n",
        "!echo $PYTHONPATH"
      ],
      "metadata": {
        "id": "zljujHpyuRRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-hydra-train task.data=\"/teamspace/studios/this_studio/datasets/santali/manifest\" \\\n",
        "    dataset.max_tokens=1000000 \\\n",
        "    common.log_interval=50 \\\n",
        "    model.freeze_finetune_updates=1000 \\\n",
        "    model.w2v_path=\"/content/indicw2v_base_pretrained.pt\" \\\n",
        "    checkpoint.save_dir=\"/content/checkpoint\" \\\n",
        "    checkpoint.restore_file=\"/content/checkpoint/checkpoint_last.pt\" \\\n",
        "    distributed_training.distributed_world_size=1 \\\n",
        "    +optimization.update_freq='[1]' \\\n",
        "    +optimization.lr=[0.00005] \\\n",
        "    optimization.max_update=100000 \\\n",
        "    checkpoint.save_interval_updates=10000 \\\n",
        "    --config-dir \"IndicWav2Vec/finetune_configs\" \\\n",
        "    --config-name ai4b_base"
      ],
      "metadata": {
        "id": "Q2ddeaCs_Osi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change --config-name to `ai4b_large` if using large model."
      ],
      "metadata": {
        "id": "UQA8PML9_eE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate\n"
      ],
      "metadata": {
        "id": "wXUuA5KgA6Bp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download and build Flashlight for decoding"
      ],
      "metadata": {
        "id": "VluX0vzKttVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flashlight-text\n",
        "!pip install git+https://github.com/kpu/kenlm.git"
      ],
      "metadata": {
        "id": "YUaPJshWA90J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/flashlight/sequence\n",
        "%cd sequence\n",
        "!pip install ."
      ],
      "metadata": {
        "id": "zQluLjrhBqDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/flashlight/sequence\n",
        "%cd sequence\n",
        "!cmake -S . -B build\n",
        "!cmake --build build --parallel\n",
        "!cd build && ctest\n",
        "%cd .. # run tests\n",
        "!cmake --install build # install at the CMAKE_INSTALL_PREFIX"
      ],
      "metadata": {
        "id": "WWBtVyZcBt7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/flashlight/text\n",
        "%cd text\n",
        "!cmake -S . -B build\n",
        "!cmake --build build --parallel\n",
        "!cd build && ctest && cd .. # run tests\n",
        "!cmake --install build # install at the CMAKE_INSTALL_PREFIX"
      ],
      "metadata": {
        "id": "gS9Ylk3BBwOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference on test/ valid set"
      ],
      "metadata": {
        "id": "GW-5n26pu2fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/IndicWav2Vec/w2v_inference/infer/infer.py \"/content/datasets/santali/manifest\" --task audio_finetuning \\\n",
        "--nbest 1 --path \"/content/gdrive/MyDrive/checkpoint_last.pt\" --gen-subset test --results-path \"/content/res/\" --w2l-decoder viterbi \\\n",
        "--lexicon none --lm-weight 0 --word-score 0 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 1000000 \\\n",
        "--post-process letter"
      ],
      "metadata": {
        "id": "SPORY7ylB-Va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference on external dataset"
      ],
      "metadata": {
        "id": "1scmbX3Bv3y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer\n",
        "!pip install Levenshtein"
      ],
      "metadata": {
        "id": "oeKDrMTGu9dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an illustration of inference of Mozila Common Voice Dataset."
      ],
      "metadata": {
        "id": "LgeTE8eMvDqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from jiwer import wer\n",
        "import subprocess\n",
        "import sys\n",
        "import re\n",
        "import Levenshtein as Lev\n",
        "\n",
        "# Load the .tsv file\n",
        "input_file = '/content/test/audio/test.tsv'  # Replace with your file path\n",
        "data = pd.read_csv(input_file, sep='\\t')\n",
        "\n",
        "def wer( s1, s2):\n",
        "        \"\"\"\n",
        "        Computes the Word Error Rate, defined as the edit distance between the\n",
        "        two provided sentences after tokenizing to words.\n",
        "        Arguments:\n",
        "            s1 (string): space-separated sentence\n",
        "            s2 (string): space-separated sentence\n",
        "        \"\"\"\n",
        "\n",
        "        # build mapping of words to integers\n",
        "        b = set(s1.split() + s2.split())\n",
        "        word2char = dict(zip(b, range(len(b))))\n",
        "\n",
        "        # map the words to a char array (Levenshtein packages only accepts\n",
        "        # strings)\n",
        "        w1 = [chr(word2char[w]) for w in s1.split()]\n",
        "        w2 = [chr(word2char[w]) for w in s2.split()]\n",
        "\n",
        "        return Lev.distance(''.join(w1), ''.join(w2))\n",
        "\n",
        "def get_trans(filepath):\n",
        "    try:\n",
        "        # Format the command\n",
        "        command = f\"\"\"\n",
        "        python /content/IndicWav2Vec/w2v_inference/scripts/sfi.py \\\n",
        "        --audio-file {filepath} \\\n",
        "        --ft-model /content/gdrive/MyDrive/checkpoint_last.pt \\\n",
        "        --w2l-decoder viterbi \\\n",
        "        --lexicon none\n",
        "        \"\"\"\n",
        "\n",
        "        # Run the command and capture output\n",
        "        result = subprocess.run(command, shell=True, text=True, capture_output=True)\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            # Successful execution, process the output\n",
        "            output = result.stdout.strip()  # Get the command's stdout and strip whitespace\n",
        "            lines = output.split('\\n')     # Split output into lines\n",
        "            prediction = lines[-1].strip()  # Get the last non-empty line\n",
        "            return prediction\n",
        "        else:\n",
        "            # Print the error message and exit\n",
        "            print(f\"Error running command: {result.stderr.strip()}\")\n",
        "            sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "wer_list = []\n",
        "\n",
        "# Iterate through each file in the 'path' column\n",
        "for index, row in data.iterrows():\n",
        "    filepath = row['path'].replace('.mp3', '.wav')  # Change .mp3 to .wav\n",
        "    actual_sentence = row['sentence']\n",
        "\n",
        "    # Get the transcript using the CLI command\n",
        "    predicted_transcript = get_trans(filepath)\n",
        "\n",
        "    # Calculate the WER\n",
        "    file_wer = wer(actual_sentence, predicted_transcript)\n",
        "    wer_list.append(file_wer)\n",
        "\n",
        "    # Debug log for current file's WER\n",
        "    print(f\"File: {filepath}, WER: {file_wer:.2%}\")\n",
        "    print(f\"Lev_WER \", wer(actual_sentence, predicted_transcript))\n",
        "    print(f\"Actual: {actual_sentence}\")\n",
        "    print(f\"Predicted: {predicted_transcript}\")\n",
        "\n",
        "# Calculate the average WER\n",
        "average_wer = sum(wer_list) / len(wer_list) if wer_list else 0\n",
        "\n",
        "# Print the average WER\n",
        "print(f\"\\nAverage WER: {average_wer:.2%}\")"
      ],
      "metadata": {
        "id": "1ScNX1D0u_6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LM integration\n",
        "\n",
        "Download and build Kenlm"
      ],
      "metadata": {
        "id": "pCNHfpjEuEuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
        "!mkdir kenlm/build\n",
        "%cd kenlm/build\n",
        "!cmake ..\n",
        "!make -j2"
      ],
      "metadata": {
        "id": "wRPL6wAxvQ4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Kenlm - Model"
      ],
      "metadata": {
        "id": "euBblM26vTyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os.path import abspath\n",
        "import os\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "\n",
        "kenlm_path = \"/content/kenlm/\"\n",
        "transcript_file = \"/content/gdrive/MyDrive/transcription.txt\"\n",
        "additional_file = \"/content/gdrive/MyDrive/corpus.txt.txt\"\n",
        "ngram = 3\n",
        "output_path = \"/content/output\"\n",
        "\n",
        "\n",
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n",
        "\n",
        "with open(transcript_file, encoding=\"utf-8\") as f:\n",
        "    train = f.read().upper().splitlines()\n",
        "    train = [' '.join(d.split()[1:]) for d in train]\n",
        "\n",
        "\n",
        "chars = [list(d.replace(' ','')) for d in train]\n",
        "chars = [j for i in chars for j in i]\n",
        "chars = set(chars)\n",
        "\n",
        "if additional_file != None:\n",
        "    with open(additional_file, encoding=\"utf-8\") as f:\n",
        "        train += f.read().upper().splitlines()\n",
        "\n",
        "vocabs = set([])\n",
        "for line in tqdm(train):\n",
        "    for word in line.split():\n",
        "        vocabs.add(word)\n",
        "vocabs = list(vocabs)\n",
        "print(len(vocabs))\n",
        "vocabs = [v for v in vocabs if not any(c for c in list(v) if c not in chars)]\n",
        "print(len(vocabs))\n",
        "\n",
        "vocab_path = os.path.join(output_path,'vocabs.txt')\n",
        "lexicon_path = os.path.join(output_path,'lexicon.txt')\n",
        "train_text_path = os.path.join(output_path,'world_lm_data.train')\n",
        "train_text_path_train = train_text_path.replace('world_lm_data.train','kenlm.train')\n",
        "model_arpa = train_text_path.replace('world_lm_data.train','kenlm.arpa')\n",
        "model_bin  = train_text_path.replace('world_lm_data.train','lm.bin')\n",
        "kenlm_path_train = os.path.join(abspath(kenlm_path) , 'build/bin/lmplz')\n",
        "kenlm_path_convert = os.path.join(abspath(kenlm_path) , 'build/bin/build_binary')\n",
        "kenlm_path_query = os.path.join(abspath(kenlm_path) , 'build/bin/query')\n",
        "\n",
        "with open(train_text_path,'w') as f:\n",
        "    f.write('\\n'.join(train))\n",
        "\n",
        "with open(vocab_path,'w') as f:\n",
        "    f.write(' '.join(vocabs))\n",
        "\n",
        "for i in range(0,len(vocabs)):\n",
        "    vocabs[i] = vocabs[i] + '\\t' + ' '.join(list(vocabs[i])) + ' |'\n",
        "\n",
        "with open(lexicon_path,'w') as f:\n",
        "    f.write('\\n'.join(vocabs))\n",
        "\n",
        "cmd = kenlm_path_train + \" -T /tmp -S 4G --discount_fallback -o \" + str(ngram) +\" --limit_vocab_file \" + vocab_path + \" trie < \" + train_text_path +  ' > ' + model_arpa\n",
        "os.system(cmd)\n",
        "cmd = kenlm_path_convert +' trie ' + model_arpa + ' ' + model_bin\n",
        "os.system(cmd)\n",
        "cmd = kenlm_path_query + ' ' + model_bin + \" < \" + train_text_path + ' > ' + train_text_path_train\n",
        "os.system(cmd)\n",
        "os.remove(train_text_path)\n",
        "os.remove(train_text_path_train)\n",
        "os.remove(model_arpa)\n",
        "os.remove(vocab_path)"
      ],
      "metadata": {
        "id": "F1OtvWmNvXw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/IndicWav2Vec/w2v_inference/infer/infer.py \"/content/datasets/santali/manifest\" --task audio_finetuning \\\n",
        "--nbest 1 --path \"/content/gdrive/MyDrive/checkpoint_last.pt\" --gen-subset test --results-path \"/content/res/\" --w2l-decoder kenlm \\\n",
        "--lexicon \"/content/output/lexicon.txt\" --kenlm-model \"/content/output/lm.bin\" --sil-weight 0 --criterion ctc --labels ltr --max-tokens 1000000 \\\n",
        "--post-process letter"
      ],
      "metadata": {
        "id": "CRlmmmvJuBJm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}